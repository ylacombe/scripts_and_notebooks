{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JGuilB2lPfiW",
        "50MASX67kT0Y",
        "OyCiuZHakT0c"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIYnTknseoSMptUAHZPryP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cffa9cd329524a3fb9c3b74548f4f1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6aa3708332f24d858fa38ed0ea55884c",
              "IPY_MODEL_6e7dfde992bf45b4b04c62e0da931568",
              "IPY_MODEL_e664badbdc3c4206bbcdfdca3710e4c8"
            ],
            "layout": "IPY_MODEL_9fd2c4c959d8457eb2a09fd0f78ffcb4"
          }
        },
        "6aa3708332f24d858fa38ed0ea55884c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79eec154ddb44b3a92a4afc32f0437c4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2a3a1db626ee4877b819b9127f9be48f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6e7dfde992bf45b4b04c62e0da931568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e95774f27b4a4e40a1dcf3eff06eaae8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf73588127174153b559a183839a9b7d",
            "value": 2
          }
        },
        "e664badbdc3c4206bbcdfdca3710e4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aaf41b25f8b4ad9936dff9b49abe24a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_188db5811f464b909ae5f5b9f2360310",
            "value": " 2/2 [00:41&lt;00:00, 20.42s/it]"
          }
        },
        "9fd2c4c959d8457eb2a09fd0f78ffcb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79eec154ddb44b3a92a4afc32f0437c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a3a1db626ee4877b819b9127f9be48f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e95774f27b4a4e40a1dcf3eff06eaae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf73588127174153b559a183839a9b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7aaf41b25f8b4ad9936dff9b49abe24a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "188db5811f464b909ae5f5b9f2360310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ylacombe/scripts_and_notebooks/blob/main/v2_seamless_m4t_hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SeamlessM4Tv2 ðŸ¤—\n",
        "\n",
        "## Goal of this notebook\n",
        "\n",
        "This notebook will teach you how to use how to easily use [SeamlessM4T v2](https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel), a foundational multimodal model for speech translation using ðŸ¤— Transformers.\n",
        "\n",
        "## TL;DR pointers\n",
        "\n",
        "1. [Installation in one line](#installation) -> `!pip install --quiet git+https://github.com/huggingface/transformers sentencepiece`\n",
        "2. [Speech to speech](#s2st)\n",
        "3. [Speech to text](#s2tt)\n",
        "4. [Text to speech](#t2ts)\n",
        "5. [Text to text](#t2tt)\n",
        "\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. [SeamlessM4T v2 docs in ðŸ¤— Transformers](https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2)\n",
        "2. [Demo on ðŸ¤— Spaces](https://huggingface.co/spaces/facebook/seamless-m4t-v2-large)\n",
        "3. [Model card](https://huggingface.co/facebook/seamless-m4t-v2-large)\n",
        "4. [Original repository](https://github.com/facebookresearch/seamless_communication)\n",
        "\n",
        "## Presentation of the model\n",
        "\n",
        "SeamlessM4T is designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text.\n",
        "\n",
        "SeamlessM4T enables multiple tasks without relying on separate models:\n",
        "\n",
        "- Speech-to-speech translation (S2ST)\n",
        "- Speech-to-text translation (S2TT)\n",
        "- Text-to-speech translation (T2ST)\n",
        "- Text-to-text translation (T2TT)\n",
        "- Automatic speech recognition (ASR)\n",
        "\n",
        "SeamlessM4T-v2 features a versatile architecture that smoothly handles the sequential generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq) models. The first model translates the input modality into translated text, while the second model generates speech tokens, known as \"unit tokens,\" from the translated text.\n",
        "\n",
        "Each modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646) architecture is placed on top of the second seq2seq model.\n",
        "\n",
        "Here's how the generation process works:\n",
        "\n",
        "- Input text or speech is processed through its specific encoder.\n",
        "- A decoder creates text tokens in the desired language.\n",
        "- If speech generation is required, the second seq2seq model, generates unit tokens in an non auto-regressive way.\n",
        "- These unit tokens are then passed through the final vocoder to produce the actual speech."
      ],
      "metadata": {
        "id": "IgF3xf7GK4Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Environment"
      ],
      "metadata": {
        "id": "JGuilB2lPfiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Throughout this tutorial, we'll use a GPU. The runtime is already configured to use the free 16GB T4 GPU provided through Google Colab Free Tier, so all you need to do is hit \"Connect T4\" in the top right-hand corner of the screen."
      ],
      "metadata": {
        "id": "gmdC1K-YSuf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <a name=\"installation\"> We just need to install the ðŸ¤— Transformers package from the main branch and the sentencepiece package:</a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "by2mT3BCPh2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet git+https://github.com/ylacombe/transformers.git@add-S2S-2 sentencepiece"
      ],
      "metadata": {
        "id": "xZjDWjdQPx8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0ba6a4-bf70-451e-e029-0a3600b6efdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also install the `datasets` package to have convenient examples at hand:"
      ],
      "metadata": {
        "id": "Oe9MXECjRC5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet datasets"
      ],
      "metadata": {
        "id": "NpCTkaA1RPik"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n"
      ],
      "metadata": {
        "id": "V-FlCxyjQsjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Load the model\n",
        "\n",
        "The pre-trained checkpoint can be loaded from [the pre-trained weights]((https://huggingface.co/facebook/seamless-m4t-v2-large)) on the ðŸ¤— Hugging Face Hub."
      ],
      "metadata": {
        "id": "HPW4I--ySp7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SeamlessM4Tv2Model\n",
        "\n",
        "model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "cffa9cd329524a3fb9c3b74548f4f1ff",
            "6aa3708332f24d858fa38ed0ea55884c",
            "6e7dfde992bf45b4b04c62e0da931568",
            "e664badbdc3c4206bbcdfdca3710e4c8",
            "9fd2c4c959d8457eb2a09fd0f78ffcb4",
            "79eec154ddb44b3a92a4afc32f0437c4",
            "2a3a1db626ee4877b819b9127f9be48f",
            "e95774f27b4a4e40a1dcf3eff06eaae8",
            "cf73588127174153b559a183839a9b7d",
            "7aaf41b25f8b4ad9936dff9b49abe24a",
            "188db5811f464b909ae5f5b9f2360310"
          ]
        },
        "id": "kwFOGI3aQdf5",
        "outputId": "186f8d5a-a555-4a2a-bffc-098c192cef0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cffa9cd329524a3fb9c3b74548f4f1ff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Place the model to an accelerator device if available."
      ],
      "metadata": {
        "id": "vPeXQ0wEkT0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:18:15.968422Z",
          "iopub.execute_input": "2023-10-25T10:18:15.968841Z",
          "iopub.status.idle": "2023-10-25T10:18:15.991577Z",
          "shell.execute_reply.started": "2023-10-25T10:18:15.968809Z",
          "shell.execute_reply": "2023-10-25T10:18:15.990606Z"
        },
        "trusted": true,
        "id": "l03ja7s5kT0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Processor\n",
        "\n",
        "Before everything, load `SeamlessM4TProcessor` in order to be able to pre-process the inputs. The Transformers package has a wide range of processors, so we'll use the `AutoProcessor` class than can recognize which processor to load from the repository id.\n",
        "\n",
        "The processor role here is two-sides:\n",
        "1. It is used to prepare inputs. It tokenizes the input text, i.e. to cut it into small pieces that the model can understand, and transforms the audio into a format more suitable for the model.\n",
        "2. It is used to process the model results. Here, it is used to \"detokenize\" the output, i.e. to perform the opposite operation to that described above."
      ],
      "metadata": {
        "id": "50MASX67kT0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:23:10.549641Z",
          "iopub.execute_input": "2023-10-25T10:23:10.550012Z",
          "iopub.status.idle": "2023-10-25T10:23:17.418417Z",
          "shell.execute_reply.started": "2023-10-25T10:23:10.549982Z",
          "shell.execute_reply": "2023-10-25T10:23:17.417429Z"
        },
        "trusted": true,
        "id": "L9Z0pPZZkT0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can seamlessly use this model on text or on audio, to generated either translated text or translated audio.\n",
        "\n",
        "### Preparing audio\n",
        "Here is how to use the processor to process audio. Here, we'll use an audio taken from an Hindi speech corpus.\n",
        "\n",
        "**Note that you don't need to specify the source language, it will be automatically understood by the model!**\n"
      ],
      "metadata": {
        "id": "MxCZ4Z4EkT0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load an audio sample from an Hindi speech corpus\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"google/fleurs\", \"hi_in\", split=\"train\", streaming=True)\n",
        "audio_sample = next(iter(dataset))[\"audio\"]\n",
        "\n",
        "print(f\"Sampling rate: {audio_sample['sampling_rate']}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:40:48.525709Z",
          "iopub.execute_input": "2023-10-25T10:40:48.526502Z",
          "iopub.status.idle": "2023-10-25T10:40:50.811746Z",
          "shell.execute_reply.started": "2023-10-25T10:40:48.526464Z",
          "shell.execute_reply": "2023-10-25T10:40:50.810788Z"
        },
        "trusted": true,
        "id": "VCwGjqwUkT0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The [sampling rate](https://en.wikipedia.org/wiki/Sampling_(signal_processing)) of the input audio must 16 kHz. If your sampling rate is different, you'll need an additional step to prepare the input audio.\n",
        "\n",
        "**Here is how to do it:**\n",
        "```python\n",
        "# we need an additional library\n",
        "# you can install it with:\n",
        "# !pip install torchaudio\n",
        "import torchaudio, torch\n",
        "\n",
        "# you need to convert the audio from a numpy array to a torch tensor first\n",
        "audio = torch.tensor(audio_sample[\"array\"])\n",
        "\n",
        "# now downsample the audio\n",
        "audio = torchaudio.functional.resample(audio, orig_freq=audio_sample['sampling_rate'], new_freq=model.config.sampling_rate)\n",
        "```"
      ],
      "metadata": {
        "id": "4g752C4akT0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use the processor:"
      ],
      "metadata": {
        "id": "YVazD4WjkT0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\").to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:42:24.494478Z",
          "iopub.execute_input": "2023-10-25T10:42:24.495177Z",
          "iopub.status.idle": "2023-10-25T10:42:24.555922Z",
          "shell.execute_reply.started": "2023-10-25T10:42:24.495141Z",
          "shell.execute_reply": "2023-10-25T10:42:24.554778Z"
        },
        "trusted": true,
        "id": "xhYQn6rekT0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing text\n",
        "\n",
        "It is much easier to prepare text, you just have to give it to the processor, alongs with the language of the text. Here the text is in English so we'll set `src_lang=\"eng\"`."
      ],
      "metadata": {
        "id": "nUKVx2XLkT0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now, process some English test as well\n",
        "text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\").to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:42:25.340088Z",
          "iopub.execute_input": "2023-10-25T10:42:25.340469Z",
          "iopub.status.idle": "2023-10-25T10:42:25.346108Z",
          "shell.execute_reply.started": "2023-10-25T10:42:25.340436Z",
          "shell.execute_reply": "2023-10-25T10:42:25.345193Z"
        },
        "trusted": true,
        "id": "DVt1FghFkT0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model usage\n",
        "\n",
        "Now, we got everything ready to actually use the model !"
      ],
      "metadata": {
        "id": "08fQYH5CkT0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate translated speech\n",
        "\n",
        "`SeamlessM4TModel v2` can *seamlessly* generate text or speech with few or no changes. Let's target Russian voice translation:"
      ],
      "metadata": {
        "id": "sgs48tB6kT0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n",
        "audio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:42:26.798576Z",
          "iopub.execute_input": "2023-10-25T10:42:26.798969Z",
          "iopub.status.idle": "2023-10-25T10:42:29.992265Z",
          "shell.execute_reply.started": "2023-10-25T10:42:26.798912Z",
          "shell.execute_reply": "2023-10-25T10:42:29.991265Z"
        },
        "trusted": true,
        "id": "bFPhzG09kT0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With the exact same code but different inputs, Iâ€™ve translated English text and Hindi speech to Russian speech samples.**\n",
        "\n",
        "Now, let's listen to the generated audios!\n",
        "\n",
        "#### From text"
      ],
      "metadata": {
        "id": "3eP8ASa3kT0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sample_rate = model.config.sampling_rate\n",
        "Audio(audio_array_from_text, rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:42:29.994152Z",
          "iopub.execute_input": "2023-10-25T10:42:29.994523Z",
          "iopub.status.idle": "2023-10-25T10:42:30.004320Z",
          "shell.execute_reply.started": "2023-10-25T10:42:29.994488Z",
          "shell.execute_reply": "2023-10-25T10:42:30.003257Z"
        },
        "_kg_hide-input": false,
        "trusted": true,
        "id": "YyTTRAL1kT0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From audio"
      ],
      "metadata": {
        "id": "_VCR5SBxkT0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(audio_array_from_audio, rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:42:30.191251Z",
          "iopub.execute_input": "2023-10-25T10:42:30.191620Z",
          "iopub.status.idle": "2023-10-25T10:42:30.203794Z",
          "shell.execute_reply.started": "2023-10-25T10:42:30.191588Z",
          "shell.execute_reply": "2023-10-25T10:42:30.202935Z"
        },
        "trusted": true,
        "id": "uzPrt7sekT0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also save audio as .wav files using a third-party library, e.g. scipy (note here that we also need to remove the channel dimension from our audio tensor):"
      ],
      "metadata": {
        "id": "g6tCWm6zkT0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "\n",
        "scipy.io.wavfile.write(\"seamless_m4t_out.wav\", rate=sample_rate, data=audio_array_from_text) # audio_array_from_audio"
      ],
      "metadata": {
        "id": "RbgOvu_OnIsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Generate translated text\n",
        "\n",
        "Similarly, you can generate translated text from audio files or from text with the same model. You only have to pass `generate_speech=False` to `SeamlessM4Tv2Model.generate`.\n",
        "\n",
        "This time, let's translate the Hindi audio to English (I personnaly don't speak Hindi ðŸ¤—) and the English text to French."
      ],
      "metadata": {
        "id": "efxmQbVMkT0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from audio\n",
        "output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "print(f\"Translation from audio: {translated_text_from_audio}\")\n",
        "\n",
        "# from text\n",
        "output_tokens = model.generate(**text_inputs, tgt_lang=\"fra\", generate_speech=False)\n",
        "translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "print(f\"Translation from text: {translated_text_from_text}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:47:30.829070Z",
          "iopub.execute_input": "2023-10-25T10:47:30.830278Z",
          "iopub.status.idle": "2023-10-25T10:47:31.444136Z",
          "shell.execute_reply.started": "2023-10-25T10:47:30.830233Z",
          "shell.execute_reply": "2023-10-25T10:47:31.443133Z"
        },
        "trusted": true,
        "id": "-eNw0zpYkT0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediary conclusion\n",
        "\n",
        "Now you know how to use [SeamlessM4T using ðŸ¤— Transformers](https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2)\n",
        "\n",
        "Let's wrap it up:\n",
        "1. SeamlessM4T v2 can **translate text/speech to text/speech.**\n",
        "2. It **supports numerous languages** and is a great step towards reducing language barriers in AI.\n",
        "3. It is **fast and efficient**.\n",
        "4. The rest of this notebook will share some **tips** on how to best use the model! (**Spoiler:** You can also do batching inference!)\n",
        "5. You can try this new version of SeamlessM4T in this [demo on ðŸ¤— Spaces](https://huggingface.co/spaces/facebook/seamless-m4t-v2-large).\n",
        "\n",
        "**Don't hesitate to share how you think this model should be used!**"
      ],
      "metadata": {
        "id": "t4YbbESMkT0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tips\n"
      ],
      "metadata": {
        "id": "OyCiuZHakT0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Use dedicated models\n",
        "\n",
        "`SeamlessM4Tv2Model` is a Transformers top level model to generate speech and text, but you can also use dedicated models that perform the task without additional components, thus reducing the memory footprint.\n",
        "For example, you can replace the audio-to-audio generation snippet with the model dedicated to the S2ST task, the rest is exactly the same code:\n",
        "\n",
        "```python\n",
        ">>> from transformers import SeamlessM4Tv2ForSpeechToSpeech\n",
        ">>> model = SeamlessM4TForSpeechToSpeech.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "```\n",
        "\n",
        "Or you can replace the text-to-text generation snippet with the model dedicated to the T2TT task, you only have to remove `generate_speech=False`.\n",
        "\n",
        "```python\n",
        ">>> from transformers import SeamlessM4Tv2ForTextToText\n",
        ">>> model = SeamlessM4TForTextToText.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "```\n",
        "\n",
        "Feel free to try out `SeamlessM4Tv2ForSpeechToText` and `SeamlessM4Tv2ForTextToSpeech` as well.\n",
        "\n",
        "#### 2. Change the speaker identity\n",
        "\n",
        "You have the possibility to change the speaker used for speech synthesis with the `speaker_id` argument. Some `speaker_id` works better than other for some languages!\n"
      ],
      "metadata": {
        "id": "jnHvPNltSiBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test with let say speaker_id=5 and tgt_lang=\"eng\"\n",
        "audio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"eng\", speaker_id=5)[0].cpu().numpy().squeeze()\n",
        "\n",
        "Audio(audio_array_from_audio, rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:55:03.547978Z",
          "iopub.execute_input": "2023-10-25T10:55:03.548683Z",
          "iopub.status.idle": "2023-10-25T10:55:05.441817Z",
          "shell.execute_reply.started": "2023-10-25T10:55:03.548650Z",
          "shell.execute_reply": "2023-10-25T10:55:05.440734Z"
        },
        "trusted": true,
        "id": "glzxCxUskT0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3. Change the generation strategy\n",
        "\n",
        "You can use different [generation strategies](./generation_strategies) for speech and text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, speech_do_sample=True)` which will successively perform beam-search decoding on the text model, and multinomial sampling on the speech model.\n"
      ],
      "metadata": {
        "id": "0jg-T80_kT0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"eng\", speaker_id=7, text_num_beams=4, speech_do_sample=True, speech_temperature=0.6)[0].cpu().numpy().squeeze()\n",
        "\n",
        "Audio(audio_array_from_audio, rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:56:31.872262Z",
          "iopub.execute_input": "2023-10-25T10:56:31.873019Z",
          "iopub.status.idle": "2023-10-25T10:56:33.893199Z",
          "shell.execute_reply.started": "2023-10-25T10:56:31.872983Z",
          "shell.execute_reply": "2023-10-25T10:56:33.892275Z"
        },
        "trusted": true,
        "id": "rFFSc8tlkT0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4. Generate speech and text at the same time\n",
        "\n",
        "Use `return_intermediate_token_ids=True` with [`SeamlessM4TModel`] to return both speech and text !"
      ],
      "metadata": {
        "id": "OoSLQnsWkT0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**audio_inputs, return_intermediate_token_ids=True, tgt_lang=\"eng\", speaker_id=7, text_num_beams=4, speech_do_sample=True, speech_temperature=0.6)\n",
        "\n",
        "audio_array_from_audio = output[0].cpu().numpy().squeeze()\n",
        "text_tokens = output[2]\n",
        "translated_text_from_text = processor.decode(text_tokens.tolist()[0], skip_special_tokens=True)\n",
        "print(f\"TRANSLATION: {translated_text_from_text}\")\n",
        "\n",
        "Audio(audio_array_from_audio, rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T11:02:25.310732Z",
          "iopub.execute_input": "2023-10-25T11:02:25.311125Z",
          "iopub.status.idle": "2023-10-25T11:02:27.621343Z",
          "shell.execute_reply.started": "2023-10-25T11:02:25.311094Z",
          "shell.execute_reply": "2023-10-25T11:02:27.620333Z"
        },
        "trusted": true,
        "id": "G0Qm8tCBkT0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Use batching for increased throughput\n",
        "\n",
        "Batching with SeamlessM4T is supported in ðŸ¤— Transformers. Here is an example with two French sentences translated to English!"
      ],
      "metadata": {
        "id": "4QxbiHvNkT0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_inputs = processor(text = [\"J'aime HF de tout mon coeur.\", \"La vie est belle.\"], src_lang=\"fra\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "audio_array_from_text = model.generate(**text_inputs, tgt_lang=\"eng\", speaker_id=7, num_beams=5, speech_do_sample=True, speech_temperature=0.6)"
      ],
      "metadata": {
        "id": "ix3hZYxSjr9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When batching, you can get the length of each generated waveform by accessing `audio_array_from_text[1]`."
      ],
      "metadata": {
        "id": "oVxA1LNQkT0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first sentence\n",
        "length = audio_array_from_text[1][0]\n",
        "audio = audio_array_from_text[0][0]\n",
        "Audio(audio[:length].cpu().numpy().squeeze(), rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T11:09:58.382184Z",
          "iopub.execute_input": "2023-10-25T11:09:58.383129Z",
          "iopub.status.idle": "2023-10-25T11:09:58.394798Z",
          "shell.execute_reply.started": "2023-10-25T11:09:58.383085Z",
          "shell.execute_reply": "2023-10-25T11:09:58.393741Z"
        },
        "trusted": true,
        "id": "nM8Rpe8UkT0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# second sentence\n",
        "length = audio_array_from_text[1][1]\n",
        "audio = audio_array_from_text[0][1]\n",
        "Audio(audio[:length].cpu().numpy().squeeze(), rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T11:10:09.684788Z",
          "iopub.execute_input": "2023-10-25T11:10:09.685173Z",
          "iopub.status.idle": "2023-10-25T11:10:09.694371Z",
          "shell.execute_reply.started": "2023-10-25T11:10:09.685139Z",
          "shell.execute_reply": "2023-10-25T11:10:09.693453Z"
        },
        "trusted": true,
        "id": "HRxHyfUdkT0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Summary\n"
      ],
      "metadata": {
        "id": "0P8wAaZ-TNKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This last part will group together the various code snippets to enable you to use the model even more easily"
      ],
      "metadata": {
        "id": "wu3Rtj2QTf7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"s2st\"> Speech to translated speech</a>"
      ],
      "metadata": {
        "id": "JceNdFYLTh_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`audio_sample[\"array\"]` is an audio waveform that have been loaded [here](#preparing-audio) using `datasets`. You can replace it with your own one-dimensional audio waveform numpy array.\n",
        "  "
      ],
      "metadata": {
        "id": "GC8zsKnOUrxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SeamlessM4Tv2Model, AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "\n",
        "# process input\n",
        "audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\").to(device)\n",
        "\n",
        "# generate translation\n",
        "audio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()"
      ],
      "metadata": {
        "id": "EL6huWAxUY8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now, let's listen to the generated audios"
      ],
      "metadata": {
        "id": "TMy41WxHVTFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sample_rate = model.config.sampling_rate\n",
        "Audio(audio_array_from_audio, rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:42:29.994152Z",
          "iopub.execute_input": "2023-10-25T10:42:29.994523Z",
          "iopub.status.idle": "2023-10-25T10:42:30.004320Z",
          "shell.execute_reply.started": "2023-10-25T10:42:29.994488Z",
          "shell.execute_reply": "2023-10-25T10:42:30.003257Z"
        },
        "_kg_hide-input": false,
        "trusted": true,
        "id": "-DiCwknpVTF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also save audio as .wav files using a third-party library, e.g. scipy (note here that we also need to remove the channel dimension from our audio tensor):\n",
        "\n",
        "```python\n",
        "import scipy\n",
        "\n",
        "scipy.io.wavfile.write(\"seamless_m4t_out.wav\", rate=sample_rate, data=audio_array_from_audio)\n",
        "```"
      ],
      "metadata": {
        "id": "kuvdtQeRVTF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"s2tt\"> Speech to translated text</a>"
      ],
      "metadata": {
        "id": "_GYmxoWOTs1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SeamlessM4Tv2Model, AutoProcessor\n",
        "\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "\n",
        "# process input\n",
        "audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\").to(device)\n",
        "\n",
        "# generate translation\n",
        "output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "print(f\"Translation from audio: {translated_text_from_audio}\")"
      ],
      "metadata": {
        "id": "JL1pWdIlV3dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"t2ts\"> Text to translated speech</a>"
      ],
      "metadata": {
        "id": "7OkyPWjVTu4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SeamlessM4Tv2Model, AutoProcessor\n",
        "\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "\n",
        "# process input\n",
        "text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "# generate translation\n",
        "audio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()"
      ],
      "metadata": {
        "id": "ILv8991wVmN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now, let's listen to the generated audios!"
      ],
      "metadata": {
        "id": "C4XxhU9gVmOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sample_rate = model.config.sampling_rate\n",
        "Audio(audio_array_from_text, rate=sample_rate)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T10:42:29.994152Z",
          "iopub.execute_input": "2023-10-25T10:42:29.994523Z",
          "iopub.status.idle": "2023-10-25T10:42:30.004320Z",
          "shell.execute_reply.started": "2023-10-25T10:42:29.994488Z",
          "shell.execute_reply": "2023-10-25T10:42:30.003257Z"
        },
        "_kg_hide-input": false,
        "trusted": true,
        "id": "eRvBck9GVmOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also save audio as .wav files using a third-party library, e.g. scipy (note here that we also need to remove the channel dimension from our audio tensor):\n",
        "\n",
        "```python\n",
        "import scipy\n",
        "\n",
        "scipy.io.wavfile.write(\"seamless_m4t_out.wav\", rate=sample_rate, data=audio_array_from_text)\n",
        "```"
      ],
      "metadata": {
        "id": "Z9YtIRXoVmOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"t2tt\"> Text to translated text</a>"
      ],
      "metadata": {
        "id": "kJzM_HcrTxm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SeamlessM4Tv2Model, AutoProcessor\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
        "\n",
        "# process input\n",
        "text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "# generate translation\n",
        "output_tokens = model.generate(**text_inputs, tgt_lang=\"fra\", generate_speech=False)\n",
        "translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "print(f\"Translation from text: {translated_text_from_text}\")"
      ],
      "metadata": {
        "id": "KskoAOhOTKma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}